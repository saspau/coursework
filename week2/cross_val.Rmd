---
title: "Interpretation and Cross Validation"
output: html_document
---

```{r setup, include = FALSE}
# import libraries
library(ggplot2)
library(tidyverse)
library(dplyr)
```

```{r load-data}
# read in data
oj <- read.csv('oj.csv')
```

### 1. Let's return to the orange juice dataset and investigate how store demographics are related to demand.
#### a. Take the "fully interacted" model from HW2 `(logmove ~ log(price)*brand*feat)` and add in the store demographics as linear features (e.g. + demo1 + demo2+.). 
```{r 1-a}
model <- lm(formula = logmove ~ log(price) * brand * feat + AGE60 + EDUC + 
               ETHNIC + INCOME + HHLARGE + WORKWOM + HVAL150 + SSTRDIST + 
               SSTRVOL + CPDIST5 + CPWVOL5, data = oj)
summary(model)
```

#### b. What demographics are significantly (t>2) related to demand? 

AGE60, EDUC, ETHNIC, INCOME, HHLARGE, HVAL150, SSTRDIST, SSTRVOL, CPDIST5, CPWVOL5

#### c. How much did the adjusted R-squared improve with the addition of these variables?
```{r 1-c}
# original model
original_model <- lm(formula = logmove ~ log(price) * brand * feat, data = oj)
summary(original_model)
```

The adjusted R-squared improved by 0.0496 with the addition of the store demongraphic variables.




### 2. Let's focus on two variables HVAL150 ("percent of HHs with homes >$150K") and one of your choosing. 

#### a. What are the means and percentiles of each of these variables? HINT: `summary(oj$HVAL150)`
```{r 2-a}
# HVAL150 summary
summary(oj$HVAL150)
exp(summary(oj$HVAL150)["Median"])

# EDUC summary
summary(oj$EDUC)
exp(summary(oj$EDUC)["Median"])
```

#### b. Using your coefficient estimates from the regression in 1b:
i. If we move from the median value of HVAL150 to the 75th percentile (3rd quartile), how much does log(quantity) change each week on average? HINT: using `coef(reg_output)["var_name"]` exports the coefficient on "var_name" from the regression model "reg_output".   Similarly, `summary(df$var_name)` will output a bunch of summary statistics for the variable var_name in data frame df.  Using `summary(df$var_name)["3rd Qu."]` will take the level of the 3rd quantile from the summary of var_name.  
Because we estimate things in logs you'll want to take the exponent of everything.
```{r 2-b}
model <- lm(formula = logmove ~ log(price) * brand * feat + AGE60 + EDUC + 
               ETHNIC + INCOME + HHLARGE + WORKWOM + HVAL150 + SSTRDIST + 
               SSTRVOL + CPDIST5 + CPWVOL5, data = oj)
#summary(model)

# for HVAL150
intercept <- coef(model)[["(Intercept)"]]
delta <- summary(oj$HVAL150)[["3rd Qu."]] - summary(oj$HVAL150)[["Median"]]
HVAL150 <- coef(model)[["HVAL150"]]
exp(delta*HVAL150)

```


ii.	If we move from the median value of HVAL150 to the 75th percentile (3rd quartile), how much does log(quantity) change each week on average?
```{r 2-b-ii}
# for EDUC
intercept <- coef(model)[["(Intercept)"]]
delta <- summary(oj$EDUC)[["3rd Qu."]] - summary(oj$EDUC)[["Median"]]
EDUC <- coef(model)[["EDUC"]]
exp(delta*EDUC)
```


iii.	Based on this analysis, which is the more important predictor of demand?

HVAL150 is slightly more important, as it demonstrated a greater change in quantity.


#### c.	Now let's see if these variables impact price sensitivity. Add two interaction terms (with logprice) to the model to test this.  (Do this quickly.) 
```{r 2-c}
model <- lm(formula = logmove ~ log(price) * brand * feat + HVAL150*log(price) + 
              EDUC*log(price) + AGE60 + EDUC + ETHNIC + INCOME + HHLARGE + 
              WORKWOM + HVAL150 + SSTRDIST + SSTRVOL + CPDIST5 + CPWVOL5, data = oj)
summary(model)
```
i.	What are the coefficients on the interaction terms?

log(price):HVAL150                2.554478
log(price):EDUC                  -0.835447


ii.	Recall, positive values indicate lower price sensitivity and negative values indicate greater price sensitivity. Do your estimates make sense based on your intuition?

iii.	What are the coefficient estimates on the constants HVAL150 and your variable of choice? How do they compare to your regression from 1b?

new regression:
HVAL150                          -1.622375  
EDUC                              1.482920

old regression:
HVAL150                           0.357561
EDUC                              0.955931


iv.	Similar to 2b, if we move from the median value of each variable to the 3rd quartile, how much does elasticity change? Based on this, which is more important to price sensitivity?
```{r 2-c-iv}
intercept <- coef(model)[["(Intercept)"]]
delta <- summary(oj$HVAL150)[["3rd Qu."]] - summary(oj$HVAL150)[["Median"]]
HVAL150 <- coef(model)[["HVAL150"]]
delta*HVAL150

intercept <- coef(model)[["(Intercept)"]]
delta <- summary(oj$EDUC)[["3rd Qu."]] - summary(oj$EDUC)[["Median"]]
EDUC <- coef(model)[["EDUC"]]
delta*EDUC
```


### 3. Tuna fish question! Create make a new dataframe which takes the previous week's prices as a variable on the same line as the current week.  This would enable you to see if there is intertemporal substitution. 
a.	There are going to be a couple of steps.  First is creating a new dataframe which is like the old one except that the week variable will change by a single week
```{r 3-a}
df1 <-oj
df1$week <- df1$week+1  
# df1 now has NEXT week and not the current one.  If we merge this by #weeks now, this is last week's price (e.g., "lagged price").

myvars <- c("price", "week", "brand", "store")
df1 <- df1[myvars]
lagged <- merge(oj, df1, by=c("brand","store","week")) 

```
Investigate the Df2 and rename the lagged store values needed for a lagged price within the same store

b.	Now run a regression with this week's log(quantity) on current and last week's price.

c.	What do you notice about the previous week's elasticity?  Does this make sales more or less attractive from a profit maximization perspective?  Why?


### 4. In the last assignment you calculated the MSE on a test set.  Let's expand that code to include 5-fold cross validation. 
#### a.	Create 5 partitions of the data of equal size.
```{r 4-a}
set.seed(1)

num_folds <- 5

# new df with shuffled rows
rand_oj <- oj[sample(nrow(oj)), ]
rand_oj$rand_idx <- seq(1, nrow(rand_oj))

# which fold does each row belong to?
rand_oj$which_fold <- rand_oj$rand_idx %% num_folds + 1

# to store MSE for each fold
MSEs <- c(1:num_folds)

# do k-fold cross-validation
for (i in 1:num_folds) {
  test <- rand_oj[which(rand_oj$which_fold == i), ]
  train <- anti_join(rand_oj, test)
  
  model <- lm(formula = logmove ~ log(price) * brand * feat + AGE60 + EDUC + 
               ETHNIC + INCOME + HHLARGE + WORKWOM + HVAL150 + SSTRDIST + 
               SSTRVOL + CPDIST5 + CPWVOL5, data = train)
  
  test$pred <- predict(model, newdata = test)
  MSE <- mean((test$pred - test$logmove)^2)
  MSEs[i] <- MSE
}

mean(MSEs)

# declare MSE array
# randomize (with sample)
# give row numbers
# mod (to get 'folds')

# loop through 'folds'
## get test (where fold == i)
## get train (anti join)
## fit model
## predict
## calculate MSE, store it 

```

#### b. Create 5 training datasets using 80% of the data for each one.  This can be done multiple ways (e.g., "appending" the data together using rbind, randomly creating partitions and sub-setting data according to them, etc.)

```{r 4-b}
```

#### c. Estimate a complex model using OLS which includes price, featured, brand, brand*price and lagged price, all the sociodemographic variables and interactions of EDUC and HHSIZE with price on each of the training sets then the MSE on the test sets using the predict command.
```{r 4-c}
```
i.	Calculate the MSE for each run of the model by averaging across all the MSEs.



### 5. Now take that same model from (4) and estimate it with LASSO.  Here is some relevant code:
```{r}
# x <- model.matrix(~ log(price) + feat + brand + brand*log(price) + . +log(lagged_price) , data= my_awesome_data) 
# y <- as.numeric(as.matrix(lagged$logmove)) 
# set.seed(720) 
# lasso_v1 <- glmnet(x, y, alpha=1)
# plot(laxxo_v1)
# coef(lasso_v1, s=lasso_v1$lambda.min)

#The cross validated version of the model (with some different objects) is this one: 
#lasso_v1 <- cv.glmnet(x, y, alpha=1)
#cvfit$lambda.min
#coef(cvfit, s = "lambda.min")
```

```{r reshaping}
oj_reshaped <- oj %>% select(store, brand)
                             
oj_reshaped <- oj_reshaped %>% spread(brand, price)
oj_merged <- merge(oj, oj_reshaped)

mm <- oj_merged %>% filter(brand == "minute.maid")
reg_mm <- glm(logmove ~ log(dominicks) + log(minute.maid) + log(tropicana), data = mm)
summary(reg_mm)

# log(minute.maid): 10% increase in mm price => 38% decrease in quantity sold
# log(dominicks): 10% increase in dominicks => 9% increase in quantity of mm sold
# log(tropicana): 10% increase in tropicana => 12% increase in quantity of mm sold


trop <- oj_merged %>% filter(brand == "tropicana")
reg_mm <- glm(logmove ~ log(dominicks) + log(minute.maid) + log(tropicana), data = trop)
summary(reg_mm)

dom <- oj_merged %>% filter(brand == "dominicks")
reg_mm <- glm(logmove ~ log(dominicks) + log(minute.maid) + log(tropicana), data = dom)
summary(reg_mm)
```
